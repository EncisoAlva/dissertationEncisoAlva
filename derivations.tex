\section{Derivation of the Estimator}


 %{Derivation of MAP estimator}
    %The estimator is derived from minimizing the a posteriori probability,
\begin{align*}
    \ppar{ \hat{\U}, \hat{\N} } &=
    \argmax_{\U, \N }\,
    \log P\ppar{\ppar{ {\U}, {\N} } \Big| {\Y; \gamma_0, \gamma_2} }
    \\
    &=
    \argmax_{\U, \N }\, \sum_{t=1}^T 
    \log P\ppar{\ppar{ {\U_t}, {\N_t} } \Big| {\Y_t; \gamma_0, \gamma_2} }
    \\
    &=
    \argmax_{\U, \N }\, \sum_{t=1}^T  \left[
    \log P\ppar{{ \Y_t } \Big| { \ppar{\U_t, \N_t}  ; \gamma_0, \gamma_2} }
    \right.
    \\
    &\phantom{=}
    \left.
    +
    P\ppar{{ {\U_t}  ; \gamma_0, \gamma_2} }
    +
    P\ppar{{ {\N_t}  ; \gamma_0, \gamma_2} }
    \right]
    \\
    &=
    \argmax_{\U, \N }\, \sum_{t=1}^T  \left[
    -\frac{1}{2\sigma^2}
    \nnorm{\G L_S \U_t + \G \N_t - \Y_t}_2^2
    \right.
    \\
    &\phantom{=}
    \left.
    -
    \frac{1}{2\gamma_0^2} \nnorm{\N_t}_2^2
    -
    \frac{1}{2\gamma_1^2} \nnorm{L_S \U_t}_2^2
    \right]
\end{align*}
 

 
\begin{align*}
\ppar{ \hat{\U}, \hat{\N} } 
&=
    \argmax_{\U, \N }\, \sum_{t=1}^T  \left[
    -\frac{1}{2\sigma^2}
    \nnorm{\G L_S \U_t + \G \N_t - \Y_t}_2^2
    \right.
    \\
    &\phantom{=}
    \left.
    -
    \frac{1}{2\gamma_0^2} \nnorm{\N_t}_2^2
    -
    \frac{1}{2\gamma_1^2} \nnorm{L_S \U_t}_2^2
    \right]
    \\
    &=
    \argmax_{\U, \N }\, \left[
    -\frac{1}{2\sigma^2}
    \nnorm{\G L_S \U + \G \N - \Y}_F^2
    \right.
    \\
    &\phantom{=}
    \left.
    -
    \frac{1}{2\gamma_0^2} \nnorm{\N}_F^2
    -
    \frac{1}{2\gamma_1^2} \nnorm{L_S \U}_F^2
    \right]
\end{align*}
 

 
This derives into the following error function
\begin{align*}
    F\ppar{ {\U}, {\N};  \gamma_0, \gamma_1} &=
    \frac{1}{2\sigma^2}
    \nnorm{\G L_S \U + \G \N - \Y}_F^2
    \nonumber \\
    &\phantom{=}
    +
    \frac{1}{2\gamma_0^2} \nnorm{\N}_F^2
    +
    \frac{1}{2\gamma_1^2} \nnorm{L_S \U}_F^2
\end{align*}
So that
\begin{align*}
\ppar{ \hat{\U}, \hat{\N} } 
&=
    \argmin_{\U, \N }\, F\ppar{ {\U}, {\N};  \gamma_0, \gamma_1}
\end{align*}
Notice that the function is convex in both $\U$ and $\N$ and thus it can be minimized iteratively over it's arguments,
    \begin{align*}
        \hat{\N}^{(i+1)} &= \argmin_{\N} F\ppar{\U^{(i)},\N; \gamma_0, \gamma_1}
        \\
        \hat{\U}^{(i+1)} &= \argmin_{\U} F\ppar{\U,\N^{(i)}; \gamma_0, \gamma_1}
    \end{align*}
 

 
Minimization is carried away via Lagrange multipliers with the constraint $\G L_S \U + \G \N - \Y=0$.
\begin{align*}
    \hat{\N}^{(i+1)} &= \argmin_{\N}  F\ppar{\U^{(i)},\N; \gamma_0, \gamma_1}
    \\
    &\phantom{=}
    \quad \quad
    \text{st} \quad \quad
    \G L_S \U^{(i)} + \G \N - \Y=0
\end{align*}
This leads to the Lagrangian function
\begin{align*}
    \mathscr{L}_\N \ppar{\N, \lambda; \U^{(i)}, \gamma_0}
    &=
    \frac{1}{2\sigma^2}
    \nnorm{\G L_S \U^{(i)} + \G \N - \Y}_F^2
    +
    \frac{1}{2\gamma_0^2} \nnorm{\N}_F^2
    \\
    &\phantom{=}
    +\lambda^T\ppar{\G L_S \U^{(i)} + \G \N - \Y}
\end{align*}
 

 
\begin{align*}
\frac{\partial}{\partial \N}
\mathscr{L}_\N \ppar{\N, \lambda; \U^{(i)}, \gamma_0}
    &=
    \frac{1}{\sigma^2}
    \G^T
    \ppar{\G L_S \U^{(i)} + \G \N - \Y}
    \\
    &\phantom{=}
    +
    \frac{1}{\gamma_0^2} {\N}
    +
    \G^T \lambda
    \\
    \frac{\partial}{\partial \lambda} 
    \mathscr{L}_\N \ppar{\N, \lambda; \U^{(i)}, \gamma_1}
    &=
    \G L_S \U^{(i)} + \G \N - \Y
\end{align*}
By setting $\frac{\partial}{\partial \N} \mathscr{L}_\N = 0$ and 
$\frac{\partial}{\partial \lambda} \mathscr{L}_\N = 0$, it arises the conditions
\begin{align*}
    {\frac{1}{\gamma_0^2} { \N}
    +
    \G^T \lambda} &= 0
    \\
    \G \N 
    &= \Y - \G L_S \U^{(i)} 
\end{align*}
 

 
\begin{align*}
&&
{\frac{1}{\gamma_0^2} { \N}
    +
    \G^T \lambda} &= 0
\\
\Rightarrow
&&
\N &= -\gamma_0^2 \G^T \lambda
\\
\Rightarrow
&&
-\gamma_0^2 \G \G^T \lambda
&=
\Y - \G L_S \U^{(i)} 
\\
\Rightarrow
&&
\lambda &=
-\frac{1}{\gamma_0^2} \spar{\G \G^T+ \frac{\gamma_0^2}{\sigma^2}^{-1}}^{-1} \ppar{\Y - \G L_S \U^{(i)} }
\\
\Rightarrow
&&
\N &=
\G^T \spar{\G \G^T+ \frac{\gamma_0^2}{\sigma^2}^{-1}} \ppar{\Y - \G L_S \U^{(i)} }
\end{align*}
Thus concluding the following
\begin{align*}
\N^{(i+1)} &=
\G^T \spar{\G \G^T+ \frac{\gamma_0^2}{\sigma^2}^{-1}} \ppar{\Y - \G L_S \U^{(i)} }
\end{align*}
 

% {MAP estimator, closed form}
%The cost function is minimized column-wise, resulting in
%\begin{align}
%    \hat{\J}(t) &= \spar{\G^T \SIG \G + \ppar{\id-A} \GAM^2 }^{-1} \G^T \SIG \Y(t),
%\end{align}
%which in turn is trivially stacked to get an estimator of $\J$,
%\begin{align}
%    \hat{\J} &= \spar{\G^T \SIG \G + \ppar{\id-A} \GAM^2 }^{-1} \G^T \SIG \Y
%\end{align}
%with $A = L_S \ppar{L^T L}^{-1} L^T$ an averaging operator.%
%
%This is a linear estimator since $\hat{\J} = \W \Y$ for some matrix.
% 

 {Maximum A Posteriori (MAP) estimator}
For the iterative step, consider the following,
\begin{align*}
\N^{(i+1)} &=
\G^T \spar{\G \G^T+ \frac{\gamma_0^2}{\sigma^2}^{-1}} \ppar{\Y - \G L_S \U^{(i)} }
\\
\N^{(i)} &=
\G^T \spar{\G \G^T+ \frac{\gamma_0^2}{\sigma^2}^{-1}} \ppar{\Y - \G L_S \U^{(i-1)} }
\\
\Rightarrow
\N^{(i+1)}-\N^{(i)} &=
\G^T \spar{\G \G^T+ \frac{\gamma_0^2}{\sigma^2}^{-1}} \G L_S \ppar{ \U^{(i)}-\U^{(i-1)} }
\end{align*}
 

 {Maximum A Posteriori (MAP) estimator [repeated slide]}
    The iterative minimization of the error function can be interpreted as a multi-scale iterative correction
    \begin{align}
        \hat{\N}^{(i+1)} &= \argmin_{\N} F\ppar{\U^{(i)},\N; \gamma_0, \gamma_1}
        \\
        \hat{\U}^{(i+1)} &= \argmin_{\U} F\ppar{\U,\N^{(i)}; \gamma_0, \gamma_1}
    \end{align}
    These steps have the following closed form
    \begin{align}
        \hat{\N}^{(i+1)} &=
        \hat{\N}^{(i)}
        -
        \G^T \spar{\G \G^T + \frac{\gamma_0^2}{\sigma^2} \id}^{-1} L_S \ppar{\hat{\U}^{(i)}-\hat{\U}^{(i-1)} }
        \\
        \hat{\U}^{(i+1)} &=
        \hat{\U}^{(i)}
        -
        L_S^T \G^T \spar{\G L_S L_S^T \G^T + \frac{\gamma_1^2}{\sigma^2} \id}^{-1} \ppar{\hat{\N}^{(i)}-\hat{\N}^{(i-1)} }
    \end{align}
 